GDELT Data Synchronization

Two programs handle GDELT data synchronization for Plato. They are
gfetch.sh and gsync.  gfetch.sh is a bash script. It downloads GDELT
2.0 downloads, concatenates them into a daily csv file, and generally
manages the files.  The downloads are done a day at a time. All the
downloads are rooted in a specific directory. For the plato server
I'm setting it up with that directory to be ~steve/gsync/gdelt. Within
this directory, there is a directory per day of the form YYYYMMDD,
a "day directory".

Within these day directories all the GDELT files for a particular day
are loaded and kept in the "zipped" state.  When all the files for
a particular day have been downloaded, they are unzipped, then
concatenated into a single CSV file. Then the gsync program is called
to process the CSV file and store the metrics into the SQL database.

The GDELT files are large, so they are not kept by default. Once gsync
has completed, everything all of the unzipped files, the zipped files,
and the concatenated CSV file are deleted. The only thing that , a
 

Handling A Multi-Day Update

The code is best utilized when processing information to update the
database 1 day at a time, cleaning up after each day is processed. Here
is the high level flow for a multi-day update, from date1 to date2:

    * make a url list for all the GDELT files that cover date1 to date2
    * for each day in the update time period
        * pull down all the *.gkg.csv.zip files for that day
        * unzip them and concatenate into a single csv file
        * delete all the unzipped files for the day, keep the zip files.
        * call gsync to process the single csv file and update the SQL db
          with the new metrics
        * if gsync runs without errors, delete all the .zip files for
          that day. Leave the .csv file and the log file in the direcory
          for that day.
    * repeat for the remaining days

So, as an example usage, suppose we want to update or verify the SQL
database for all the GCAM data between April 15, 2024 and April 22, 2024.
Let's say that you have a directory, ./gdelt, and this is where you will
always run the sync process (highly recommended to do it this way). Here's
the command we would run:

    gfetch.sh -d1 20240415 -d2 20240422

Internally, this is what gfetch.sh will do. First, it make sure that 
masterlist.txt is in the current directory. If it's not there, it will
be downloaded.  (You can do this download by running gfetch.sh -m).

Next, it will create a file containing the list of URLs to download based on
the daterange to download. For this example it will run:

    gsync -d1 20240415 -d2 20240422 >urls.txt

These URLs will be extracted from masterlist.txt. So, if masterlist.csv
is older than April 22, 2024 it will need to be updated, so gfetch may
need to download masterlist.txt even if you already have a copy.
gfetch will use 'tail -1 masterlist.txt' to determine the date of the
latest URL. If the latest URL is prior to the -d2 date, then it will
download masterlist.txt

Next, we run gfetch will begin to process the URLs in urls.txt. It will
create a directory in ./gdelt named YYYYMMDD for the year, month, and
day it will be processing. So, for example, the first URL it reads from
the list will be something like this:

    http://data.gdeltproject.org/gdeltv2/20240415000000.gkg.csv.zip

So, it will look at the first 8 characters of the file name "20240415".
It will then create ./gdelt/20240415 if it does not already exist, then
it will download all the files containing "20240415" into that directory.
Once it sees a url where the first 8 characters of the filename are
different, for example "20240416", it will pause the downloading and process
./gdelt/20240415 .  It will unzip all the zipped files, then concatenate
them together to produce 20240415.csv.   This is exactly what happens
if you run gfetch.sh -C 20240415 (internally, gfetch calls the same
function).  Once this completes it calls gsync to process this directory

    gsync -gf 20240415 > gsync-20240415.log

When this completes it would continue by processing the URLs for
April 16, 2024, that is, those URLs with the first 8 characters of their
filename being "20240416". And this process would continue until all the
urls in urls.txt have been processed.




GDELT Data Synchronization Overview

Components

The GDELT data synchronization for the Plato project is managed by
two primary programs: gfetch.sh and gsync.

gfetch.sh

This Bash script orchestrates the entire data synchronization
process. It is responsible for downloading GDELT 2.0 data files,
organizing these files into daily CSV formats, and managing the
directories and files used by gsync to extract and process the data.
It logs everything it does and saves the log files.

gsync

Implemented in Go, this program processes the daily CSV files created
by gfetch.sh and updates Plato's SQL database with new metrics. A
log for each gsync run is saved, and gsync is run with the -verbose
option to ensure detailed logging.

Workflow

gfetch.sh operates within a specified base directory. If no directory
is provided, it defaults to creating and using a directory named
"gdelt" in the current working directory. The script is capable of
handling multiple days' data; however, it processes downloads one
day at a time to manage system resources effectively.  Each day's
data is stored in a dedicated "day directory" named in the YYYYMMDD
format within the base directory. These directories contain all the
GDELT files for the respective day in their zipped form. Once all
files for a day are downloaded, gfetch.sh unzips them, concatenates
them into a single CSV file, and invokes gsync to process this file
and write the metric values to Plato's SQL database.  By default,
once gsync has processed the data, gfetch.sh cleans up by deleting
all the original zipped and unzipped files, as well as the concatenated
CSV, leaving only the gsync log file in the day directory. For
initial operations on the Plato server, the -k option is used to
retain the zipped files and the CSV for debugging purposes. With
the -k option turned on, the day directories typically consume 2-3
GB.

Automation

A cron job is set up to run gfetch.sh daily at 3:00 AM in
/home/steve/gsync. This job synchronizes the previous day's data
and checks the data for the past six days to ensure accuracy and
consistency. When the cron job completes, it also creates a CSV
database file set and publishes these files on Plato's website,
making it easy to get your own local copy of the most up-to-date
data.

Syncing Multiple Days

gfetch.sh is optimized for sequentially processing multiple days
of data, ensuring that the SQL database is updated one day at a
time and then cleaned up. This methodical approach is detailed in
the steps below:

URL List Creation

Generate a list of URLs for the GDELT files spanning from the start
date (-b) to the end date (-e).

Daily Processing

* Download all .gkg.csv.zip files for the day.

* Unzip and concatenate them into a single CSV file.

* Delete all unzipped files, retaining the zipped files if specified.

* Use gsync to update the SQL database with the new metrics from
the CSV.

* Upon successful update, delete the zipped files, leaving the CSV
and log files.

* Repeat for each day in the specified range.

Example Usage

To update the SQL database with GCAM data from April 15, 2024, to
April 22, 2024, within the ./gdelt directory, use the following
command: gfetch.sh -b 20240415 -e 20240422

This command tells gfetch.sh to manage data synchronization by
ensuring all necessary data from the GDELT project is accurately
downloaded, processed, published to Plato's SQL database, and
verified, maintaining an organized structure in the specified
directory.

Appendix A.  Translated News Articles

We discovered a discrepancy between the daily values that we were
calculating and the values we get when we use Google's Big Data
infrastructure, which also has the GDELT data loaded into a SQL
database.  We tracked down the difference to the fact that GDELT
publishes its data in 2 parts.  The first part is the one we are
currently processing and it covers many news articles world-wide.
GDELT updates a masterfile every 15 minutes:

 http://data.gdeltproject.org/gdeltv2/masterfilelist.txt

 It contains pointers to datafiles it publishes during the past
 15 minutes. We are interested in the files ending in gkg.csv.zip.
 For example, here is an actual file name: 20150327081500.gkg.csv.zip
 Its format is:  YYYYMMDDHHMMSS.gkg.csv.zip.  So the example file is
 for the data published in the 15-minute time period ending on
 May 27, 2015 8:15:00am.  
 
 To briefly summarize how our synchronization works, we download all
 the .gkgcsv.zip files for a particular day (YYYYMMDD), unzip them, 
 then concatenate them into a single .csv file. We use gsync to process
 this single .csv file and it updates our sql database with all the
 relevant stats for that day (YYYYMMDD).

 As it turns out, GDELT also publishes information about articles that
 are translated to other languages for consumption... thus, the same
 articles are exposed to more people in different languages. This
 will obviously impact sentiment values, and other time-series data
 that GDELT publishes.  From the perspective of our synchronization 
 program, it means that we need to include this information as we
 calculate our values.  So, we need to add the translated file information
 into our aggregated .csv file that gets processd by gsync.  The masterfile
 for the translated artiles is here:

 http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt

 Its format and contents are just like what we get in the other masterfile.
 The only difference is that it points to the files with information about
 the translated articles.  Basically, we just need to download those files
 in addition to the ones we are already downloading, and unzip them, then they
 will be concatenated into the single csv file we process with gsync.
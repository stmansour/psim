The PLATO Simulator Design

Revised: March 29, 2024

Imagine an investment engine that leverages both financial data and
real-world events to predict currency movements. That's the core
idea behind the PLATO Project. While the underlying approach using
genetic algorithms has the potential to be applied to other financial
markets as well, PLATO is currently focused on predicting investment
strategies in foreign currency markets.

The primary goal is to develop a prediction engine that analyzes
historical data (traditional econometrics) and incorporates current
events (via linguistic metrics) to forecast currency value changes.
The PLATO system centers around two main types of simulated entities:
Investors and Influencers. Genetic algorithms are then used to
identify optimal configurations for these entities within a reasonable
timeframe.

PLATO takes an innovative approach to discover successful investment
strategies. We create simulated investors, each with a starting pot
of capital. These investors are aided by simulated influencers who
provide insights to guide their investment decisions.

The core of PLATO is a program that acts as a simulated market.
This program, called the simulator, runs over a specified period
of historical data. Each investor is given a virtual fund in one
currency and the ability to buy and sell another currency.  Investors
make decisions daily, based on historical information available up
to that point in the simulation, mimicking real-world trading
limitations.

At the end of the simulation period, the program calculates the
annualized return for each investor.  PLATO then uses a genetic
algorithm, inspired by natural selection, to create new generations
of investors. Investors with stronger performance have a higher
chance of influencing the next generation.  This iterative process
continues for a set number of generations, with the goal of evolving
a population of investors with increasingly successful strategies.

The simulator offers extensive settings and controls, all configured
through a JSON5 file. Currently, there is no graphical user interface
for the Plato system; it is managed via text editors and scripts.
The entire system was designed to be highly scriptable in terms of
configuration, usage, and output. The configuration file allows for
detailed customization of simulations, Investors, and Influencers,
ensuring that almost every conceivable variable can be tailored to
specific needs.

The Core Classes: Influencers and Investors

The 'Influencer' class is one cornerstone of the architecture,
acting as a base for a variety of derived classes. Each subclass
is designed to analyze a distinct metric. The metrics fall into two
categories: econometric indicators (such as Inflation Rate, Housing
Starts, Discount Rate, Stock Indexes, and Oil Prices) and linguistic
metrics derived from news articles (such as sentiment, lexical, and
thematic analysis) with data sourced from the GDELT project.
Influencers are tasked with predicting the movement of currency
values between two predefined currencies, C1 and C2, recommending
one of three actions: "Buy", "Hold", or "Sell". Currently, the
predictions include a probability and weighting factor, both of
which are defaulted to 1 until a workable method for their determination
is developed.

Influencer subclasses have specific internal settings that affect
the accuracy of their predictions. For example, in forecasting the
movement for a target date T3, an Influencer can examine historical
data within a specified window, from T3-Delta1 to T3-Delta2. These
parameters, Delta1 and Delta2, are crucial as they define the
timeframe for data analysis. Though an Influencer’s examination of
the data at this time is very limited, we are in the process of
updating it to consider more data within that time range, and
balancing this with performance concerns (see Appendix B). While
external constraints are applied to these values to improve
computational efficiency and focus on relevant data, the genetic
algorithm does the heavy lifting to fine-tune Delta1 and Delta2 to
optimal values. We are confident that there are many opportunities
to improve an Influencer’s analysis of data.

The way that an Influencer makes its prediction is one of the areas
of the Plato system that needs further analysis. We believe there
is a great opportunity to improve the predictive accuracy of the
Influencers if we apply better statistical analysis on its data.
This is discussed further in Appendix E.

The second cornerstone class is the 'Investor', tasked with managing
its own private team of Influencers, and an investment fund. Its
investment fund is initialized at the start of each new generation
to a predefined value (InitFunds), denominated in a primary currency,
C1. The Investor's goal is to maximize its fund by trading C1 for
a secondary currency, C2, and back, based on recommendations from
its team of Influencers. One implementation restriction that has
been put into place is that for a given metric (be it econometric
or linguistic) an Investor can have only a single Influencer for
that metric.  We don’t know whether this is a good or bad limitation,
but it does limit the problem space that is defined in Appendix B.

On a given date, the Investor solicits predictions from its Influencers
to inform its potential action: buying C2 with C1, or selling C2
for C1, or holding its current positions. Investment decisions hinge
on a predetermined internal value, StdInvestment, the default
investment amount when conditions favor buying. These decisions are
guided by a COAStrategy (Course Of Action Strategy), with two primary
strategies implemented: "Majority Rules" and "Distributed Decision."

Majority Rules: A straightforward approach where the Investor acts
based on the majority recommendation from Influencers. A majority
for 'buy' translates to converting StdInvestment of C1 into C2; a
majority for 'sell' means converting all C2 holdings back to C1.
In the event of a tie between 'buy' and 'sell' recommendations, the
Investor maintains its current holdings.

Distributed Decision: This strategy adjusts the action's intensity
based on the proportion of recommendations. For example, if 6 out
of 10 Influencers suggest selling, the Investor sells 60% of its
C2 holdings. A buy recommendation from 7 out of 10 Influencers leads
to purchasing C2 with 70% of the StdInvestment. The strategy also
accounts for 'hold' recommendations, adjusting the action proportionally.
For instance, if the recommendations are split with 4 buys, 3 holds,
and 3 sells, the net result favors a reduced buy action, converting
40% of StdInvestment into C2, based on the differential between
buy, hold, and sell votes (4/(4+3+3) = 40%). A tie occurs when the
number of buy recommendations equals the number of sell recommendations.
When a tie occurs, the Investor's action will be to hold.

The composition, number, and configuration of Influencers associated
with an Investor, and even the strategy used by the Investor, are
optimized using genetic algorithms. That is, the process of creating
a new generation is influenced primarily by the most successful
Investors of the prior generation.  Instead of trying to go through
every possible combination of Influencers and every possible
configuration of each of their settings, we let the genetic algorithms
find the ones that work.  Although it's possible to manually (or
even programmatically) configure specific Investors with designated
Influencers, the genetic algorithms effectively find the composition
and settings that work well over successive generations.

An Investor's performance in generating profit is influenced not
only by its team of Influencers but also by various other parameters.
Essentially, the identity of an Investor is characterized by the
constellation of its Influencers, their respective configurations,
and the Investor's own settings. This framework allows for the
creation of a vast number of distinct Investors, each with the
potential to achieve optimal investment outcomes. To ascertain the
most effective configurations, we use a simulation program designed
to evaluate a population of unique Investors. The efficacy of each
Investor is measured by the amount of money it made during the
simulation period. The simulator uses historical econometric data
and incorporates linguistic metrics derived from news articles,
magazines, and other relevant sources to account for real-world
events, sentiments, and narratives that could influence currency
exchange rates. It also applies industry-standard fees to transactions.
For transactions occurring at time T, the Influencers and Investors
have access to historical data up to time T, as well as the linguistic
metrics up to time T.  This approach ensures that the costs and
information available reflect real-life conditions to the greatest
extent possible. Those Investors that accrue significant profits
are deemed superior, whereas those that lose money are classified
as inferior. This performance is reflected in the genetics code by
the Fitness Score, which is calculated for every Investor just
before creating a new generation. While the nitty-gritty details
of how this is done vary with some of the configuration options
that we provide in Plato, the basic calculation is simple: the more
money an Investor makes, the higher its Fitness Score will be. The
genetics code uses the Fitness Score during the process of creating
a new generation. In our implementation, each new Investor created
for generation n has two “parents” from generation n-1. Like
biological parents, the new Investor will inherit traits from its
parents. The new Investor’s parents are selected at random, but
those with higher Fitness Scores are more likely to be selected.
So, the traits of Investors in a given generation primarily reflect
the traits of the best Investors from the previous generation. Over
successive generations, the Investor’s traits become more and more
refined.

The Plato system uses DNA much like biological DNA. In Plato, DNA
strings provide a complete representation of an Investor, its
settings, its team of Influencers, and their settings. An Investor
and its team of Influencers and all their settings can be fully
reconstituted from the DNA string.

There is one randomizing factor that is critical in the genetics
process that can change things for better or worse: “mutation”. In
biology, mutation results from errors in DNA replication during
cell division or from viral infections.  In the Plato system,
mutation occurs at a random rate, which is set to 1% by default but
can be adjusted. The current implementation of mutation occurs to
1% (or the current setting) of the new Investors after they have
been created. In the mutation process, one aspect of the new Investor
is changed completely at random. The change is not based on any
trait from its parents, it is a completely random change.  It could
be a change to the Investor’s Influencer team. For example, an
Influencer may be deleted from the team, or one may be added to the
team. Or an Influencer may be changed from one type to another. For
example, an Influencer that looked at a country’s Inflation Rate
may be changed to an Influencer that looks at the linguistics
happiness sentiment. Or it could be that the value of an Influencer’s
setting is changed – for example, its Delta1 or Delta2 value is
changed. Or it could be that one of the Investor’s settings is
changed. For example, its decision-making process might be changed
from Distributed Decision to Majority Rules. Mutation is an important
feature of the genetic process. Without it, successive generations
may get stuck and never come upon a better solution to the problems
they encounter.

Note that the genetic process does not guarantee that a given
generation will do better at achieving its goal than the previous
generation. There is still randomness involved. But our experience
is that the capabilities of the average Investor improve dramatically
over just a few generations. How well a population can do is largely
constrained by the amount of diversity that appears in the initial
generation. The challenges of creating a diverse initial population
are discussed in Appendix B.

There is a special mode of operation in the simulator called "the
crucible".  By feeding the simulator a curated list of DNA strings
along with specified time periods, the crucible reconstitutes each
of the Investors and subjects them to a series of investment
simulations across the designated time periods. This process Makes
it easy to test the adaptability and robustness of each Investor's
strategy across varying market conditions. This targeted approach
allows us to isolate and analyze the effectiveness of top-performing
Investors, ensuring that only the most resilient and profitable
Investors are used in actual investing. When we configure the time
periods for crucible testing we can embed the notion of a moveable
window in time relative to the current system date. This makes it
much easier to test the DNA over time because we don’t need to
change the date ranges.  That is, in a config file, we can specify
date ranges for the crucible as shown below:

    "CruciblePeriods": [
      {"Duration": "3m", "Ending": "yesterday"}, {"Duration": "6m",
      "Ending": "today"}, {"Duration": "1y", "Ending": "lastMonthEnd"},
      {"Duration": "1y", "Ending": "2023-12-31"}, { "DtStart":
      "2015-06-01", "DtStop": "2015-09-30", }, { "DtStart":
      "2015-09-01", "DtStop": "2015-12-31", },
    ],

The crucible, therefore, stands as a critical component of the
system, bridging the gap between theoretical optimization and
practical, real-world application.

"Single Investor Mode" is another special operational mode within
the simulator.  In this mode, the simulation focuses exclusively
on a single Investor, defined by its DNA. This mode allows the
simulator to run through the designated simulation period for only
this Investor, resulting in very focused and minimal output reports.
This makes it practical to perform a deeper analysis.  The "Single
Investor Mode" can be combined with the "trace" mode to provide a
complete picture of the decision-making process of the Investor and
its team of Influencers. The trace mode breaks down every decision
made by the investment team. It shows the metric data processed by
each Influencer and its prediction based on that data.  It also
shows the overall decision made by the Investor based on the
predictions from its team of Influencers. If a buy or sell is
performed, it shows the transaction details as well as the total
amount of C1 and C2 after the transaction.  This facilitates
comprehensive code validation.  It serves as an invaluable debugging
and optimization tool, enabling both developers and researchers to
scrutinize the performance of Influencers as well as an Investor's
strategy, ensuring everything operates as intended and contributes
effectively to the Investor's performance.

The simulator has several different reports that provide insight
into how the code is working and the performance of the Investors.
The reports are all in CSV format to help facilitate experimentation,
research, and graphing. Here are a few of the key reports.

simstats.csv - The Simulation Statistics report provides key insights
into the effectiveness of the genetic algorithm. It tracks various
metrics across generations, including the percentage of Investors
that become profitable by the end of the simulation period. This
profitability rate is a crucial indicator of the algorithm's success.
In our observations, the percentage of profitable Investors often
starts low (around 20% - 50%) in early generations, but then rises
significantly, reaching 85-90% after just 4 or 5 generations. By
generation 10, simulations typically see the percentage of profitable
Investors high 90s, with some generations even achieving 100% of
the Investor population achieving profits. This rapid increase
strongly suggests that the genetic algorithm is effectively identifying
and breeding successful Investor configurations. The report also
lists the DNA of the most profitable Investor for each generation.

finrep.csv - The Financial Report. Key information in this report
includes the maximum annualized return for the top n Investors
across all generations of the simulation, the generation in which
that Investor appeared, and its DNA.

crep.csv - The Crucible Report. The simulator is fed a list of DNA
strings and a separate list of time periods.  For each DNA string
provided, it reconstitutes the Investor and runs an investment
simulation for that individual Investor over all the time periods
provided. It then summarizes the results similar to the Financial
Report.

invrep.csv - The Investment Report.  This report provides a detailed
transaction log for one or more Investors. It lists all details of
every exchange, the exchange rates, and for sells (exchanging C2
for C1) it lists the profit or loss on that transaction. If the
Investor's investment strategy is Distributed Decision then it is
likely that a buy (an exchange of C1 for C2) will be sold in chunks.
This report accounts for every chunk sold.

There are other modes and capabilities of the simulator. A few key
ones are the following:

Top Investor Count Feature: This configurable option allows users
to specify the number of top-performing Investors for which detailed
reports are generated. The simulator will keep track of the
top-performing investors across all generations. That is, if the
user configures a simulation for 50 generations, with 1000 Investors
per generation, and sets the Top Investor Count to 10, then at the
end of the simulation the financial report will include the 10 best
Investors over all 50 generations, their financial performance, and
will also include the DNA of each top-performing Investor.

Preserve Elite Option: The Preserve Elite option ensures that a
specified percentage of the highest-performing Investors from one
generation are exactly replicated in the subsequent generation.
Though this feature diverges from traditional genetic algorithm
practices, we have found it to enhance the overall performance of
our Investor pool.

Investor Bonus Plan: Mimicking real-world incentive structures when
enabled, this feature boosts an Investor's genetic Fitness Score
based on its performance.  Investors surpassing certain profitability
thresholds receive a Fitness Score multiplier, beginning at 2X and
scaling up with their ROI achievements. This elevation in Fitness
Score directly influences their likelihood of contributing to the
genetic makeup of future generations, prioritizing the propagation
of successful investment strategies.

Gen 0 Elite Injection: When enabled, this optional feature introduces
the DNA of exceptionally successful Investors into the initial
generation. This provides a strategic advantage right from the
start. When this feature is disabled, the first generation's
population is entirely random. Incorporating proven genetic material
early on effectively primes the simulation with a foundation of
excellence, offering a significant "head start" in the evolutionary
process.

Appendix A: Challenges in Assessing Influencer Accuracy

It's difficult to measure how accurate each type of Influencer is
at predicting future market movements. Here's why:

Variable Lookback Periods:  Influencers consider different amounts
of historical data (lookback periods) when making predictions. These
lookback periods are set by the genetic algorithm and can change
significantly.  This makes it hard to say something like "Influencer
X is right 80% of the time." Instead, we could say "Influencer X,
when looking back at 90 days (Delta1) and 19 days (Delta2) of data,
predicted correctly 80% of the time between January 1st, 2022 and
December 31st, 2022." However, Delta1 can range from -145 to -60
days and Delta2 can range from -45 to -1 days. So, there are many
possible combinations (3740 in this case) for these settings, and
the accuracy could be different for each combination. Also, the
accuracy for this Influencer with these settings would likely be
different if we looked at data from a different time period (e.g.,
2023).

Uncertain Selling Dates:  Even if an Influencer predicts a correct
"buy," it's hard to know how accurate it was because we don't know
if that Influencer would recommend selling on the day it is decided
to sell the C2 it recommended to buy.  For example, let's say
Influencer X suggests buying currency C2 at time T1.  If the Investor
buys, but then later decides not to sell when Influencer X recommends
it (T2), we can't say if Influencer X's buy recommendation was good
or bad (we don’t track that info).  Similarly, if Influencer X
recommends holding at T2 but the Investor sells and makes money,
can we really say Influencer X was right?  These situations make
it difficult to link Influencer predictions directly to how much
money the Investor makes or loses.

Complex Sales:  A single "buy" prediction that leads to a purchase
of X amount of C2 might lead to multiple sells in which portions
of X are sold at different times.  The multiple sell transactions
make it hard to track a specific sell amount back to a specific buy
recommendation from an Influencer to measure the accuracy of the
prediction.  Again, Influencer A may have recommended the buy, but
there’s no guarantee that every portion of X that is sold came from
Influencer A recommending that sale. So, the attribution of success
or failure back to Influencer A can become very messy.

These challenges highlight the complexity of evaluating Influencer
accuracy within the Plato Project simulation.

We may need to look at other ways to determine how good Influencers
are. For example, we might look for any commonality in which
Influencers appear in the most successful Investors.

Appendix B: Grasping Plato’s Problem Of Scale

Plato’s approach to investing is not based on a deep understanding
of the world of exchange rates. Instead, it is based on our belief
that patterns over time in econometrics and linguistic metrics can
be combined and interpreted in ways that will forecast relative
changes in currency exchange values. The current Plato system
implementation is far enough along for us to put this to the test
in a very small way.  We have done simulations using generations
consisting of thousands of Investors utilizing teams of Influencers.
We have DNA for Investors that have earned 27% annualized return
in the years 2022 and 2023.  While we are encouraged by these
results, there may be  Investors who could achieve much higher
returns, perhaps 35%, or 50%, or more. We don’t know. We would like
to find them if they exist. But it is important to grasp, at some
level, the number of possible Investor configurations that exist.
This will help us give us perspective on the results we achieve and
help us prioritize the work we still need to do.

An early prototype program to test the genetics for its efficacy
toward this problem was given a much simpler problem to solve
(https://stevemansour.com/code/p5/genetics1/index.html). For this
problem, there happens to be a perfect solution. If a member of the
generation achieved a Fitness Score of 1, it embodied the perfect
solution. So the simple simulator program to test was built to
simply keep iterating generations until it produced the “perfect”
member of the population. It’s been tested hundreds of times, and
with the generation size set to 500 it generally takes 500 to 3000
generations to find the perfect member, and the time required is
generally less than a minute.

The problem we’re solving for Plato is many orders of magnitude
more complicated than the prototype test program.  The test program
problem had approximately 1.285 * 10^79 possibilities and only one
of them was correct. For Plato, we do not know what the perfect or
best possible solution is. So, we must search for Investors that
make the highest returns. Fortunately, it is easy to describe what
makes one solution better than another… the higher the annualized
return on an Investor’s initial fund the better.  So, the more money
an Investor has in its fund at the end of the simulation period,
the higher its Fitness Score will be. On the downside, we don’t
know exactly how many metrics could be highly relevant.  As of this
writing, we have 161 metrics, each with its own Influencer. So,
even now our problem is on the order of 161 factorial, that is  ~
7.59 * 10^286, possible Influencer teams. That number is incomprehensible.
And remember, each member of the team has many thousands of variations
because its parameters can be tuned. But, more important, the number
of metrics is growing rapidly and may grow to a thousand or more.
This would bring the number of possible Investor teams to around
1000 factorial (incomprehensively greater than the incomprehensible
161 factorial).

On a MacBook Pro with an M1 processor and 64GB RAM, here are some
simulation times for a varying population of Investors who have
been constrained to have Influencer team sizes between 1 and 36.

  Population    Simulation Time per Generation ----------
  ------------------------------
	       100                 0 sec 557 msec
       1,000                 5 sec 375 msec
      10,000                54 sec 920 msec
     100,000          9 min 34 sec
   1,000,000    1 hr 32 min 51 sec

There may exist Investors who could achieve 35%, 50%, or even higher
returns. Even with the assistance of the genetic algorithm, it is
unclear how much time we would need to spend to find them.

We must search for ways to speed up our simulations to find better
Investors in a shorter period of time. We must look for ways to
parallelize simulations.


Appendix C:  Performance Considerations

The amount of data needed by the simulator to hold multiple year’s
worth of time-based metrics is relatively small. While the simulator
can use either an SQL database or a CSV database for this information,
the CSV data is loaded into memory in its entirety. Using the CSV
database is essentially running the simulator with all its data
cached. As a result, it runs several orders of magnitude faster
than using the SQL database as a datasource.  While this approach
helps, we would like to look at opportunities to parallelize the
operation of the simulator.

One approach would be to parallelize the processing of an Investor’s
daily run. During the run each of the Investor’s Influencers are
asked for their prediction. Thus, each of them will make several
calls to the data layer to provide time-series data for the metric
that they forecast with. This operation is currently serialized.
The simulator manages a list of Investors. It calls each Investor
serially to handle its daily affairs.  If this operation could be
parallelized we may be able to speed things up.  It is unclear how
the performance will go in that they all use the same datastore.

Update: I’ve created a “thread pool” that can run a day’s simulation
across multiple CPU cores. All the cores access the single in-memory
cached CSV database for metrics. I've tried various simulations,
all the cores are vying for access to the internal CSV data store.
That limits the effectiveness of multiple cores. Our server has 120
CPU cores.  Our peak performance on the plato server occurs when
we use between 25 and 30 cores. Overall, the thread pool has
significantly improved performance. I’ve also done profiling on the
code and removed several issues that caused the memory allocation
code to overwork. This also improved performance slightly.



Appendix D:  Influencer Prediction Logic

Each Influencer is tasked with making predictions about whether to
buy, hold, or sell currency C2.  This section of the document looks
in detail at how this is done.  Each Influencer subtype looks at a
specific metric. The methodology it uses to determine how to predict
is the same, regardless of the metric. We begin by describing the
relevant dates. Influencers are asked to make a buy-sell-hold
prediction for time T3. Recall that two parameters of every Influencer
are Delta1 and Delta2, both of which are negative numbers.  They
are used to calculate T1 and T2.  T1 = T3 + Delta1.  T2 = T3 +
Delta2. Their values are such that T1 < T2 < T3.  As a simple
example, Delta1 could be -30 and Delta2 could be -7.  This means
that T1 is about a month before T3, and T2 is a week prior to T3.
The time period from T1 to T2 is where the Influencer examines its
metric to make its prediction.  There are currently 4 predictive
options for how this is handled in the Influencer:


SingleValGT SingleValLT C1C2RatioGT C1C2RatioLT

Each Influencer is configured in the MISubclasses SQL table, or in
misubclasses.csv in the CSV database to use one of these predictive
algorithms. By defining this (and some other) configuration in a
database table, we can add new metrics and new Influencers without
changing the source code. So, we do not need to re-release the
software to add, remove, or change Influencers.

We’ll start by explaining the simplest one called “Single Value
Greater-Than” or “SingleValGT” for short. When the GetPrediction(t3)
method for an Influencer is called, the Influencer first calculates
T1 and T2 as described above. Next, it gets its metric for those
dates, val1 and val2.  Next, it computes delta, the change in value:

delta = val2 - val1

Finally, it decides whether the delta value warrants an action.  A
“hold window” is defined. The hold window defines a positive value
Hpos and a negative value Hneg. We are still experimenting with how
these numbers are defined, so I will not list a specific calculation
at this time. But the motivation for it is easy to understand.  If
delta is within the hold window, that is Hneg <= delta <= Hpos, the
Influencer will recommend “hold”.  If the delta > Hpos then it will
recommend “sell”. And if delta < Hneg it will recommend “buy”.

We have recently created a new way of calculating Hpos and Hneg.
We calculate the Standard Deviation of the metric over the N days
ending on T2 (N = 365 currently, but it is settable), and we define
a damping factor X (X = 0.1 currently, but it is settable). We look
for changes greater in magnitude than the damping factor times the
Standard Deviation. Specifically we set Hneg = -(X * Stardard
Deviation) and Hpos = X * Standard Deviation, so it is centered
around 0.

There is much more to do with the predictions, and I will update
this section as we make progress.

Appendix E: Improving Influencer Predictions on Ratio Metrics

Influencers have been implemented with a very simplistic prediction
capability to allow us to progress quickly through the development
of the entire system. The intent is to circle back and refine the
prediction logic with a tried-and-true statistical analysis of the
metric. In Appendix D we discussed the general approach to how
Influencers predict. However, it was focused on metrics where the
value of the metric is assumed to have some amount of influence on
an Exchange Rate between two currencies.  There is another type of
comparison that we do which is also likely to impact the Exchange
Rates. This involves looking at the ratios of a metric associated
with an exchange rate’s locale. As an example, suppose we have C1
= US Dollar and C2 = Japanese Yen. Let’s suppose that the metric
we look at is the Housing Starts metric associated with each local.
So, HS = Housing Starts metric, USDHS = Housing Starts metric for
the USA, and JPYHS = Housing Starts for Japan.  We still have Delta1
and Delta2 and they have the same meaning as described in Appendix
D to define T1 and T2.  But now, the metric that we look at is
USDHS/JPYHS at T1 and T2.

The question now is do we calculate the Standard Deviation of this
ratio over the last N days? Are there other things we should be
considering, such as the slope of the metric curve at T1 and T2?
These questions still need to be answered and coded.

Appendix F: Implementation Details

The Plato simulator has been implemented in Golang.  The build
system utilizes ‘make’ as the main orchestrator of the build.
Several syntax, code-checkers and linters are in place and the build
will fail if there are any issues in the source code.  Both unit
tests and functional tests also run during the build process. Tests
are written using Go’s testing infrastructure and there are also
shell scripts that perform many functional tests. The simulator
also has command-line options to turn on performance profiling and
memory profiling which can be scrutinized in Go’s profiler tool

Source code coverage is also checked in multiple places. While it
has not been a focus, the current suite of tests achieves about 70%
coverage of all the source code. The tests have been focused on
testing function and almost no work has been dedicated to improving
source code coverage numbers.

The simulator is implemented with a worker pool to distribute work
over multiple CPUs. Parallel processing is achieved with Go routines.
The worker pool utilizes a single, in-memory database in a read-only
fashion obviating the need for mutexes and locks.

With populations of 1,000,000 or greater and the number of generations
exceeding 100, even with all the parallel processing, some of the
simulations can run for days.  So, the simulator also has an HTTP
listener allowing a user to contact it while it is running, determine
its status, and even give it a command that instructs the simulator
to terminate after it completes the generation currently in progress.
In this way, even though we may need to restart the simulations
with a newer version of the code, we do not lose any of the information
the simulations produced after however many generations were run.
In the future, we may extend this capability to change parameters
in the middle of the run.

Appendix G: Managing External Resources

In the development and operation of the Plato suite, accessing
external resources frequently requires confidential credentials.
These resources may include access keys for financial institutions,
data vendors, or connections to critical services such as shared
databases. To prevent both unintentional and deliberate exposure
of these credentials, they are not stored within the source code
repository.

For secure management of these sensitive details, the Plato suite
utilizes a specific configuration file, extres.json5 (note: JSON5
is a special version of JSON that allows for comments). This file
should be located in the directory where the tool’s executable is
kept. Importantly, extres.json5 must not be checked into the version
control system to avoid compromising its contents. It should be
securely stored and only deployed in environments where it is
explicitly required, underscoring that most Plato operations do not
need access to extres.json5.

When a Plato tool initiates, it searches for extres.json5 to access
the necessary credentials. In the absence of this file, the tool
defaults to creating an ExternalResources structure with minimal
configuration: it includes the executing process owner's username
(excluding a password) and settings for a default local MySQL
database instance. This approach ensures the tool retains basic
functionality while clearly indicating when specialized external
resources are not available.

Appendix H: Database Management

The database is updated nightly automatically, every day.

The data for the econometrics and for the published articles that
we get from GDELT changes constantly. Our database is updated every
night by two cron jobs that run at 2am. The first one collects all
the GDELT updates for yesterday and writes the updated information
to our SQL database.  The second one collects all the econometric
data updates from yesterday and writes the updated information to
the SQL database.

Next, a new CSV database fileset is generated from the information
in the SQL database.  The new database fileset is released to the
plato installation on the server.  Furthermore, the fileset is made
available on the plato website for download.

As explained earlier, we use the CSV fileset for simulations as it
is loaded into memory and essentially acts as a cached version of
the database. This allows simulations to run two orders of magnitude
faster compared with an implementation that used the SQL database
directly.

Appendix I: Duplicate Investor Detection and Correction

As testing of the simulator progressed, it was discovered that many
Investors were functionally equivalent. The random creation and
generation process sometimes produced Investors with the same
settings and Influencers. As the number of generations increased
and the genetic process fine-tuned the initial generation, the
number of duplicate Investors also increased. For example, one
configuration file specified a population size of 20,000. When
simulated for 1 generation, 2 duplicate Investors were found. When
run for 10 generations, this rose to 976 duplicate Investors, and
at 20 generations, it rose to 1,437 duplicate Investors.

Duplicate Investors are undesirable for two reasons. First, once
an Investor has gone through a simulation period, its results will
be the same forever. There is no need to simulate it more than once.
If that Investor achieved a high enough ranking and PreserveElite
is turned on, it is propagated to future generations solely for
participating in the production of subsequent generations. It
achieved a high rate of return in its initial generation, so it
will always achieve that same rate of return in subsequent generations.
However, the other Investors in that generation may not do as well.
Keeping one or more successful Investors in the population helps
maintain a positive impact on subsequent generations, with the hope
that subsequent generations produce even better Investors.

The calculation for Elites selects the top n percent of Investors
at the end of each generation. There is no guarantee that if an
Investor is an elite during one generation, it will also be an elite
in the following generation(s). In fact, it is beneficial if it is
not an elite in the following generations, as this indicates that
enough new Investors outperformed it, resulting in it being bumped
off the elite list. This indicates a significant improvement among
the Investors.

To detect and prevent duplicate Investors, several changes were
made to the source code. Functionally, the code guarantees the
following:

Investors within a given generation are guaranteed to be unique.
Investors across multiple generations are guaranteed to be unique,
except if PreserveElite is enabled. That is, a population of Investors
will not contain a duplicate Investor from another generation unless
it is an Elite. Note that even if PreserveElite is enabled, the
population of any given generation will still have unique Investors.
Implementing this involved two steps. The first was producing DNA
in a way that exposed functionally equivalent Investors. The DNA
for an Investor and an Influencer is generated in a well-defined,
predictable way. However, since Investors are in a list maintained
by the Investor and the DNA is generated by producing the DNA for
each Influencer in the list sequentially, two Investors with exactly
the same Influencers might have different looking DNA if the
Influencers appear in the list in a different order. To address
this, the order of the Influencers is now based on their associated
metric. The list is sorted based on the metric name, guaranteeing
that functionally equivalent lists of Influencers will always be
stored identically and produce the same DNA strings. This sorting
is done after the Investor is fully created. It must be done this
way because the genetic process for creating new Investors is, at
its core, random. It cannot produce things in a guaranteed order.

The duplication detection is done just prior to adding a newly
created Investor to the Investor list, and just after generating
its GUID.

DNA strings can potentially become very long. Therefore, the DNA
is hashed into a GUID, and all the GUIDs for every Investor across
all generations are saved in an SQLite database file for the duration
of the simulator run. When creating a new Investor, its GUID is
generated and checked against the database. If a matching GUID is
found, the Investor is discarded and a new one is generated to
ensure uniqueness. The only exception is when PreserveElite is
enabled, in which case the elite Investors are moved to the new
generation without regeneration, and the remaining new Investors
are added only if their GUID is not found in the database.

This level of checking introduces substantial overhead in time and
resources. The GUIDs are 64 bytes each and are stored in an SQLite
database file for the duration of the simulator run.

You may wonder why SQLite was chosen as the main database for the
Plato project and a CSV file was chosen for the database used for
simulations by the simulator. There are several reasons. First, the
CSV approach of keeping the data in memory was rejected because of
the size implications with large simulations. With a typical
population of 100,000 Investors (which could rise to a million or
more depending on the circumstances) and a number of generations
reaching 1,000 (or more), saving this many GUIDs would exhaust most
of the memory of the computers in use. Therefore, the use of an
ephemeral database like SQLite is warranted. And since a quick
lookup capability was needed, a simple SQL query is a good option.
Second, the SQLite database is ephemeral and requires no additional
setup by any of the users of the simulator. Setting up an installation
of MySQL or a similar database involves a lot more work. SQLite
comes installed on Macs by default. It is a single download on Linux
systems. No users need to be added to the system, making it much
simpler to support than something like MySQL. Finally, there is no
need for concurrent access to this database. So, SQLite provides a
good solution to get SQL functionality with minimal overhead.

The simulator supports a -dup option. By default, the simulator
detects and prevents duplicate Investors. However, if you want to
run the simulator without the overhead of detecting and regenerating
duplicate Investors, you can run the simulator with the -dup option,
and these checks will be skipped. In terms of timing, a simulation
with a population size of 20,000 running for 20 generations over a
1-year period required 10 minutes with duplicate checks turned on.
The same run took 8 minutes when the duplicate checks were skipped.

It is only our guess that removing duplicates is overall a better
approach than allowing them within a generation. The thinking is
that every duplicate that exists represents one less unique
combination, and one of those new unique combinations may be or
lead to an Investor that surpasses all others ever created in terms
of its performance. There's no way of knowing whether the time/space
trade-offs to achieve this level of uniqueness are worth it. It
could be argued that the overall time saved by allowing the duplicates
could allow us to do more simulations, giving us a better chance
of producing the best Investor ever.
